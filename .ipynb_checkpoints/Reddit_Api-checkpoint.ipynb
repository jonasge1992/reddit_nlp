{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42530aca-2587-47a9-9c5a-daea94b6e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import praw\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76aacae-0d86-4a1c-a456-68e6175a319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = \"Top topics in data science 1.0 by jonasge1992\"\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"41NkDPnyuZyd9XyXLhKPvQ\",\n",
    "    client_secret=\"PHQtVGrLzYUno_hmrdp3eyv7sACmTg\",\n",
    "    user_agent=user_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b8540f-7d5a-4393-b06f-f6c07df38fa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2046it [00:26, 77.65it/s]                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Subreddit                              Thread_Title  Thread_Score  \\\n",
      "0  worldnews  US: Hamas formally rejected hostage deal          1383   \n",
      "1  worldnews  US: Hamas formally rejected hostage deal          1383   \n",
      "2  worldnews  US: Hamas formally rejected hostage deal          1383   \n",
      "3  worldnews  US: Hamas formally rejected hostage deal          1383   \n",
      "4  worldnews  US: Hamas formally rejected hostage deal          1383   \n",
      "\n",
      "                                          Thread_URL  Thread_Num_Comments  \\\n",
      "0  https://www.jns.org/us-hamas-formally-rejected...                  230   \n",
      "1  https://www.jns.org/us-hamas-formally-rejected...                  230   \n",
      "2  https://www.jns.org/us-hamas-formally-rejected...                  230   \n",
      "3  https://www.jns.org/us-hamas-formally-rejected...                  230   \n",
      "4  https://www.jns.org/us-hamas-formally-rejected...                  230   \n",
      "\n",
      "       Thread_Flair Thread_Selftext  \\\n",
      "0  Israel/Palestine                   \n",
      "1  Israel/Palestine                   \n",
      "2  Israel/Palestine                   \n",
      "3  Israel/Palestine                   \n",
      "4  Israel/Palestine                   \n",
      "\n",
      "                                        Comment_Body  Comment_Score  \\\n",
      "0                                   NO WAY!!! ðŸ¤¯ðŸ¤¯ðŸ¤¯ /s            777   \n",
      "1  It's almost like hamas was the problem all along.            800   \n",
      "2          Terrorists being terroristsâ€¦.\\n\\nShocking              1   \n",
      "3                      Israel will still get blamed.            358   \n",
      "4  We didn't expect this from these nice masked f...             64   \n",
      "\n",
      "    Comment_Author  \n",
      "0  psychotimelord_  \n",
      "1          Mhdamas  \n",
      "2       HisGibness  \n",
      "3          icenoid  \n",
      "4          tenonic  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to fetch threads and comments from multiple subreddits with progress tracking and keyword filtering\n",
    "def fetch_subreddit_data(subreddit_names, num_threads_per_subreddit, search_keywords):\n",
    "    data = []\n",
    "    \n",
    "    # Initialize tqdm progress bar\n",
    "    pbar = tqdm(total=len(subreddit_names) * num_threads_per_subreddit)\n",
    "    \n",
    "    for subreddit_name in subreddit_names:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        threads = subreddit.hot(limit=num_threads_per_subreddit)  # Adjust as needed (top threads per subreddit)\n",
    "\n",
    "        for thread in threads:\n",
    "            # Skip live threads\n",
    "            if 'live' in (thread.link_flair_text or '').lower() or 'live' in thread.title.lower():\n",
    "                continue\n",
    "            \n",
    "            # Check if thread title or selftext contains any of the search keywords\n",
    "            if any(keyword.lower() in thread.title.lower() or keyword.lower() in thread.selftext.lower() for keyword in search_keywords):\n",
    "                thread.comments.replace_more(limit=None)  # Fetch all comments, including MoreComments\n",
    "\n",
    "                for comment in thread.comments.list():\n",
    "                    if isinstance(comment, praw.models.MoreComments):\n",
    "                        continue  # Skip MoreComments objects\n",
    "\n",
    "                    thread_data = {\n",
    "                        'Subreddit': subreddit_name,\n",
    "                        'Thread_Title': thread.title,\n",
    "                        'Thread_Score': thread.score,\n",
    "                        'Thread_URL': thread.url,\n",
    "                        'Thread_Num_Comments': thread.num_comments,\n",
    "                        'Thread_Flair': thread.link_flair_text if thread.link_flair_text else 'None',\n",
    "                        'Thread_Selftext': thread.selftext,\n",
    "                        'Comment_Body': comment.body,\n",
    "                        'Comment_Score': comment.score,\n",
    "                        'Comment_Author': comment.author.name if comment.author else '[deleted]'\n",
    "                    }\n",
    "                    data.append(thread_data)\n",
    "                    pbar.update(1)  # Update progress bar\n",
    "        \n",
    "    pbar.close()  # Close progress bar after completion\n",
    "    return data\n",
    "\n",
    "# Example usage: Fetch data from multiple subreddits and filter by search keywords\n",
    "subreddit_names = ['worldnews', 'Israel', 'Palestine', 'PoliticalDiscussion', \n",
    "                   'NeutralPolitics', 'MiddleEast', 'Geopolitics', 'ForeignPolicy']\n",
    "num_threads_per_subreddit = 10  # Number of top threads to fetch per subreddit\n",
    "search_keywords = ['Israel', 'Gaza', 'Palestine', 'Netanyahu', 'Hamas', 'Abbas', 'Bibi']  # Keywords to filter threads\n",
    "\n",
    "subreddit_data = fetch_subreddit_data(subreddit_names, num_threads_per_subreddit, search_keywords)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(subreddit_data)\n",
    "\n",
    "# Display the DataFrame (optional)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54fabd7a-b7d2-41ba-b6e2-babb5547165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Praw Documentation\n",
    "# https://praw.readthedocs.io/en/stable/\n",
    "# https://praw.readthedocs.io/en/latest/code_overview/models/submission.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad3a27f-3791-4b0e-92f9-c5627d391e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up of text\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Stop words present in the library\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_links(text):\n",
    "    # Define the regex pattern for URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    # Replace the URLs with an empty string\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "#defining the function to remove punctuation\n",
    "def clean_text(text):\n",
    "    punctuation_free = \"\".join([i if i not in string.punctuation else ' ' for i in text])\n",
    "    lowertext = punctuation_free.lower()\n",
    "    text = lowertext.strip()\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenization(text):\n",
    "    tokens = re.split(' ',text)\n",
    "    return tokens\n",
    "\n",
    "#defining the function to remove stopwords from tokenized text\n",
    "def remove_stopwords(text):\n",
    "    output= [i for i in text if i not in stopwords]\n",
    "    return output\n",
    "\n",
    "#importing the Stemming function from nltk library\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#defining the object for stemming\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "#defining a function for stemming\n",
    "def stemming(text):\n",
    "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
    "    return stem_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1535c041-9de8-4f35-94d6-49962bd5306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dffd0e06-3d8a-4a1d-bc90-6c10b1345aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Comment_Body_Cleaned\"] = df[\"Comment_Body\"].apply(lambda x: remove_links(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "268639fc-d423-4255-8afe-a82b20bb07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Comment_Body_Cleaned\"] = df[\"Comment_Body_Cleaned\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fdd8056-a14e-41c2-9f22-34b7cfad2247",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Comment_Body_Tokenized\"] = df[\"Comment_Body_Cleaned\"].apply(lambda x: tokenization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf2f7db4-8a07-4069-a0a9-9c99a6938f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Comment_Body_Tokenized\"] = df[\"Comment_Body_Tokenized\"].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b0c8dd0-0183-4dfe-8a31-4950448ef9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Comment_Body_Tokenized\"] = df[\"Comment_Body_Tokenized\"].apply(lambda x: lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ed6cba0-6bea-4fd4-a2a1-0844eac82b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Comment_wo_links_stopwords\"] = df[\"Comment_Body\"].apply(lambda x: remove_links(x))\n",
    "df[\"Comment_wo_links_stopwords\"] = df[\"Comment_Body\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "634175e3-113d-438b-bb29-141d16a3ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(words):\n",
    "    sentence = ' '.join(words)\n",
    "    return sentence\n",
    "df[\"Comment_Body_Cleaned\"] = df[\"Comment_Body_Tokenized\"].apply(lambda x: join_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1d793df-a056-42f9-92eb-ddc2fc1247a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "israel            683\n",
       "hamas             399\n",
       "people            314\n",
       "would             295\n",
       "like              245\n",
       "                 ... \n",
       "alreadyexactly      1\n",
       "irlanyone           1\n",
       "tale                1\n",
       "handmaiden          1\n",
       "sc                  1\n",
       "Name: count, Length: 7867, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = pd.Series(''.join(df.Comment_Body_Cleaned).split()).value_counts()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5968a490-85d4-4447-b347-f2b4c9298805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_sentence_length(sentence):\n",
    "    if len(sentence)>512:\n",
    "        sentence = sentence[:512]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47a6e641-975d-4ab5-84fd-05c7ca33d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Comment_Body_Cleaned_Shortened\"] = df[\"Comment_Body_Cleaned\"].apply(lambda x: reduce_sentence_length(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43b9c3c6-d3e4-4adf-bf91-8eea54d89ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use a pipeline as a high-level helper\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 4\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/reddit_nlp/lib/python3.10/site-packages/transformers/pipelines/__init__.py:1005\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   1003\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1005\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/reddit_nlp/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:899\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/reddit_nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2094\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2094\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2095\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2096\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2097\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2098\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2099\u001b[0m     )\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb4718f-04ae-4738-9c12-8411c6bf0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df[\"Comment_Body_Cleaned_Shortened\"].apply(lambda x: classifier(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026a45f-88d4-450a-a52e-7fbc726359c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c58295-082f-4856-8e7b-677e8c5d6077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "model.save_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "\n",
    "df[\"Comment_wo_links_stopwords_shortened\"] = df[\"Comment_wo_links_stopwords\"].apply(lambda x: reduce_sentence_length(x))\n",
    "df[\"Comment_wo_links_stopwords_shortened_tokens\"] = df[\"Comment_wo_links_stopwords_shortened\"].apply(lambda x: tokenizer(x,return_tensors=\"tf\"))\n",
    "df[\"Output\"] = df[\"Comment_wo_links_stopwords_shortened_tokens\"].apply(lambda x: model(x))\n",
    "df[\"Scores\"] = df[\"Output\"].apply(lambda x: x[0][0].numpy())\n",
    "df[\"Scores\"] = df[\"Scores\"].apply(lambda x: softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed69635-0700-4484-adf0-768c2ce16c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the label with the highest probability\n",
    "def get_highest_label(scores):\n",
    "    ranking = np.argsort(scores)[::-1]\n",
    "    highest_label_index = ranking[0]\n",
    "    highest_label = model.config.id2label[highest_label_index]\n",
    "    return highest_label\n",
    "\n",
    "# Apply the function to get the highest label for each row\n",
    "df[\"Highest_Label\"] = df[\"Scores\"].apply(lambda x: get_highest_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b4fa5-9d6f-4821-8bbf-7e96d2440b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Highest_Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf2c1b-48fc-4d3c-84c6-f4cb88ca9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unwrap 'label' from each list element into a new column\n",
    "df['label'] = result.apply(lambda x: x[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc08fc7-d19f-4247-8865-1085fc1ef2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"score\"] = result.apply(lambda x: x[0]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551dff2-ef8b-4117-b6b0-ea6fd9afc503",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb0a05-f969-40bc-b314-1b271a710007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV (optional)\n",
    "df.to_csv('filtered_subreddit_threads_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e3146e-92bc-4341-9e46-09668ae395fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd75cce8-7d22-4ff9-a93e-68f5f5f12123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7636e2-3343-4998-85bb-597ad734ef00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
