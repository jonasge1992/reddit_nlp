{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42530aca-2587-47a9-9c5a-daea94b6e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import praw\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76aacae-0d86-4a1c-a456-68e6175a319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = \"Top topics in data science 1.0 by jonasge1992\"\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"41NkDPnyuZyd9XyXLhKPvQ\",\n",
    "    client_secret=\"PHQtVGrLzYUno_hmrdp3eyv7sACmTg\",\n",
    "    user_agent=user_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b8540f-7d5a-4393-b06f-f6c07df38fa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1698it [00:25, 66.41it/s]                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Subreddit                              Thread_Title  Thread_Score  \\\n",
      "0  worldnews  US: Hamas formally rejected hostage deal          3371   \n",
      "1  worldnews  US: Hamas formally rejected hostage deal          3371   \n",
      "2  worldnews  US: Hamas formally rejected hostage deal          3371   \n",
      "3  worldnews  US: Hamas formally rejected hostage deal          3371   \n",
      "4  worldnews  US: Hamas formally rejected hostage deal          3371   \n",
      "\n",
      "                                          Thread_URL  Thread_Num_Comments  \\\n",
      "0  https://www.jns.org/us-hamas-formally-rejected...                  523   \n",
      "1  https://www.jns.org/us-hamas-formally-rejected...                  523   \n",
      "2  https://www.jns.org/us-hamas-formally-rejected...                  523   \n",
      "3  https://www.jns.org/us-hamas-formally-rejected...                  523   \n",
      "4  https://www.jns.org/us-hamas-formally-rejected...                  523   \n",
      "\n",
      "       Thread_Flair Thread_Selftext  \\\n",
      "0  Israel/Palestine                   \n",
      "1  Israel/Palestine                   \n",
      "2  Israel/Palestine                   \n",
      "3  Israel/Palestine                   \n",
      "4  Israel/Palestine                   \n",
      "\n",
      "                                        Comment_Body  Comment_Score  \\\n",
      "0                                   NO WAY!!! ðŸ¤¯ðŸ¤¯ðŸ¤¯ /s           1696   \n",
      "1                          Here's my surprised face.            347   \n",
      "2  It's almost like hamas was the problem all along.           1455   \n",
      "3          Terrorists being terroristsâ€¦.\\n\\nShocking            325   \n",
      "4  #RELEASETHEHOSTAGES\\n\\nyou absolutely miserabl...            229   \n",
      "\n",
      "    Comment_Author  \n",
      "0  psychotimelord_  \n",
      "1          oshaboy  \n",
      "2          Mhdamas  \n",
      "3       HisGibness  \n",
      "4      Newstargirl  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to fetch threads and comments from multiple subreddits with progress tracking and keyword filtering\n",
    "def fetch_subreddit_data(subreddit_names, num_threads_per_subreddit, search_keywords):\n",
    "    data = []\n",
    "    \n",
    "    # Initialize tqdm progress bar\n",
    "    pbar = tqdm(total=len(subreddit_names) * num_threads_per_subreddit)\n",
    "    \n",
    "    for subreddit_name in subreddit_names:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        threads = subreddit.hot(limit=num_threads_per_subreddit)  # Adjust as needed (top threads per subreddit)\n",
    "\n",
    "        for thread in threads:\n",
    "            # Skip live threads\n",
    "            if 'live' in (thread.link_flair_text or '').lower() or 'live' in thread.title.lower():\n",
    "                continue\n",
    "            \n",
    "            # Check if thread title or selftext contains any of the search keywords\n",
    "            if any(keyword.lower() in thread.title.lower() or keyword.lower() in thread.selftext.lower() for keyword in search_keywords):\n",
    "                thread.comments.replace_more(limit=None)  # Fetch all comments, including MoreComments\n",
    "\n",
    "                for comment in thread.comments.list():\n",
    "                    if isinstance(comment, praw.models.MoreComments):\n",
    "                        continue  # Skip MoreComments objects\n",
    "\n",
    "                    thread_data = {\n",
    "                        'Subreddit': subreddit_name,\n",
    "                        'Thread_Title': thread.title,\n",
    "                        'Thread_Score': thread.score,\n",
    "                        'Thread_URL': thread.url,\n",
    "                        'Thread_Num_Comments': thread.num_comments,\n",
    "                        'Thread_Flair': thread.link_flair_text if thread.link_flair_text else 'None',\n",
    "                        'Thread_Selftext': thread.selftext,\n",
    "                        'Comment_Body': comment.body,\n",
    "                        'Comment_Score': comment.score,\n",
    "                        'Comment_Author': comment.author.name if comment.author else '[deleted]'\n",
    "                    }\n",
    "                    data.append(thread_data)\n",
    "                    pbar.update(1)  # Update progress bar\n",
    "        \n",
    "    pbar.close()  # Close progress bar after completion\n",
    "    return data\n",
    "\n",
    "# Example usage: Fetch data from multiple subreddits and filter by search keywords\n",
    "subreddit_names = ['worldnews', 'Israel', 'Palestine', 'PoliticalDiscussion', \n",
    "                   'NeutralPolitics', 'MiddleEast', 'Geopolitics', 'ForeignPolicy']\n",
    "num_threads_per_subreddit = 10  # Number of top threads to fetch per subreddit\n",
    "search_keywords = ['Israel', 'Gaza', 'Palestine', 'Netanyahu', 'Hamas', 'Abbas', 'Bibi']  # Keywords to filter threads\n",
    "\n",
    "subreddit_data = fetch_subreddit_data(subreddit_names, num_threads_per_subreddit, search_keywords)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(subreddit_data)\n",
    "\n",
    "# Display the DataFrame (optional)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54fabd7a-b7d2-41ba-b6e2-babb5547165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Praw Documentation\n",
    "# https://praw.readthedocs.io/en/stable/\n",
    "# https://praw.readthedocs.io/en/latest/code_overview/models/submission.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad3a27f-3791-4b0e-92f9-c5627d391e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up of text\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Stop words present in the library\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_links(text):\n",
    "    # Define the regex pattern for URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    # Replace the URLs with an empty string\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "#defining the function to remove punctuation\n",
    "def clean_text(text):\n",
    "    punctuation_free = \"\".join([i if i not in string.punctuation else ' ' for i in text])\n",
    "    lowertext = punctuation_free.lower()\n",
    "    text = lowertext.strip()\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenization(text):\n",
    "    tokens = re.split(' ',text)\n",
    "    return tokens\n",
    "\n",
    "#defining the function to remove stopwords from tokenized text\n",
    "def remove_stopwords(text):\n",
    "    output= [i for i in text if i not in stopwords]\n",
    "    return output\n",
    "\n",
    "#importing the Stemming function from nltk library\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#defining the object for stemming\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "#defining a function for stemming\n",
    "def stemming(text):\n",
    "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
    "    return stem_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1535c041-9de8-4f35-94d6-49962bd5306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dffd0e06-3d8a-4a1d-bc90-6c10b1345aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Comment_Body_Cleaned\"] = df[\"Comment_Body\"].apply(lambda x: remove_links(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "268639fc-d423-4255-8afe-a82b20bb07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Comment_Body_Cleaned\"] = df[\"Comment_Body_Cleaned\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fdd8056-a14e-41c2-9f22-34b7cfad2247",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Comment_Body_Tokenized\"] = df[\"Comment_Body_Cleaned\"].apply(lambda x: tokenization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf2f7db4-8a07-4069-a0a9-9c99a6938f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Comment_Body_Tokenized\"] = df[\"Comment_Body_Tokenized\"].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b0c8dd0-0183-4dfe-8a31-4950448ef9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Comment_Body_Tokenized\"] = df[\"Comment_Body_Tokenized\"].apply(lambda x: lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ed6cba0-6bea-4fd4-a2a1-0844eac82b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Comment_wo_links_stopwords\"] = df[\"Comment_Body\"].apply(lambda x: remove_links(x))\n",
    "df[\"Comment_wo_links_stopwords\"] = df[\"Comment_Body\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "634175e3-113d-438b-bb29-141d16a3ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(words):\n",
    "    sentence = ' '.join(words)\n",
    "    return sentence\n",
    "df[\"Comment_Body_Cleaned\"] = df[\"Comment_Body_Tokenized\"].apply(lambda x: join_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e40c84d-a047-4cf6-b21b-2666c42b6de1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/.pyenv/versions/3.10.6/envs/reddit_nlp/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-06-26 18:20:18.027072: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-26 18:20:20.051627: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/jonas/.pyenv/versions/3.10.6/envs/reddit_nlp/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/jonas/.pyenv/versions/3.10.6/envs/reddit_nlp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5038b9bbf8344a648140c9b9f8177585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5684e7d583244e7d99340c8a6a1f3f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468f35f6c9754dc19ff8919cc5ba5a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac5356353c94d2294f2b6fa1c98eeb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff97543857046bfa0ff392739d9d73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb5eb14cc3d489a98271cf4291cb7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "import torch \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "classifier = pipeline(\"zero-shot-classification\", \n",
    "                      model=\"facebook/bart-large-mnli\", \n",
    "                      device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c746052-9de2-4a9f-a862-ce8c7f2827da",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = [\"Pro-Israel, Anti-Palestine\",\"Pro-Palestine, Anti-Israel\",\"Neutral between Palestine-Israel\",\"Not relevant to the Israel-Palestine conflict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "228581f4-9586-4321-b45c-274b2c88ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Combined\"] = \"Title of post: \" + df[\"Thread_Title\"] + \". Comment to be analyzed: \" + df[\"Comment_Body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e595037-468c-46ef-8ecb-2ef16449a065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title of post: US: Hamas formally rejected hostage deal. Comment to be analyzed: Terrorists being terroristsâ€¦.\\n\\nShocking'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Combined\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94eff82d-e6ec-4d6a-a71e-e468da68270d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': \"Title of post: US: Hamas formally rejected hostage deal. Comment to be analyzed: It's almost like hamas was the problem all along.\",\n",
       " 'labels': ['Pro-Palestine, Anti-Israel',\n",
       "  'Neutral between Palestine-Israel',\n",
       "  'Pro-Israel, Anti-Palestine',\n",
       "  'Not relevant to the Israel-Palestine conflict'],\n",
       " 'scores': [0.5397629737854004,\n",
       "  0.22978489100933075,\n",
       "  0.19806399941444397,\n",
       "  0.032388243824243546]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(df[\"Combined\"][2], candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0dc15f1d-3ddf-4226-9134-1e072b4b1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93f93006-7988-4aed-b4c8-a1e34078e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf[\"Combined\"] = df[\"Combined\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b603a7e-ccd8-440e-88b1-ed503a679891",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = newdf[\"Combined\"].apply(lambda x: classifier(x, candidate_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2bd169e-45c1-4bee-9498-d525ad57eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data\n",
    "sequences = []\n",
    "best_labels = []\n",
    "\n",
    "# Iterate through each item\n",
    "for item in results:\n",
    "    # Find the index of the label with the highest score\n",
    "    best_index = max(range(len(item['labels'])), key=lambda i: item['scores'][i])\n",
    "    \n",
    "    # Append sequence and best label to lists\n",
    "    sequences.append(item['sequence'])\n",
    "    best_labels.append(item['labels'][best_index])\n",
    "\n",
    "# Create a DataFrame\n",
    "new_df = pd.DataFrame({\n",
    "    'Sequence': sequences,\n",
    "    'Best Label': best_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e316c6b-700c-4ad1-ac03-da3f6f9d8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV (optional)\n",
    "new_df.to_csv('combined_text_zero_shot_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1d793df-a056-42f9-92eb-ddc2fc1247a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "israel          692\n",
       "hamas           405\n",
       "people          318\n",
       "would           295\n",
       "like            249\n",
       "               ... \n",
       "sudden            1\n",
       "antisemites       1\n",
       "cautiontrump      1\n",
       "damaging          1\n",
       "sc                1\n",
       "Name: count, Length: 7964, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = pd.Series(''.join(df.Comment_Body_Cleaned).split()).value_counts()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5968a490-85d4-4447-b347-f2b4c9298805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_sentence_length(sentence):\n",
    "    if len(sentence)>512:\n",
    "        sentence = sentence[:512]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47a6e641-975d-4ab5-84fd-05c7ca33d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Comment_Body_Cleaned_Shortened\"] = df[\"Comment_Body_Cleaned\"].apply(lambda x: reduce_sentence_length(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43b9c3c6-d3e4-4adf-bf91-8eea54d89ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffb4718f-04ae-4738-9c12-8411c6bf0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df[\"Comment_Body_Cleaned_Shortened\"].apply(lambda x: classifier(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3026a45f-88d4-450a-a52e-7fbc726359c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [{'label': 'neutral', 'score': 0.3826168775558...\n",
       "1       [{'label': 'neutral', 'score': 0.3778409957885...\n",
       "2       [{'label': 'neutral', 'score': 0.3529797792434...\n",
       "3       [{'label': 'neutral', 'score': 0.3902023732662...\n",
       "4       [{'label': 'neutral', 'score': 0.4040221571922...\n",
       "                              ...                        \n",
       "2084    [{'label': 'neutral', 'score': 0.3796155154705...\n",
       "2085    [{'label': 'positive', 'score': 0.358612775802...\n",
       "2086    [{'label': 'neutral', 'score': 0.3696939647197...\n",
       "2087    [{'label': 'positive', 'score': 0.361505061388...\n",
       "2088    [{'label': 'neutral', 'score': 0.3721854388713...\n",
       "Name: Comment_Body_Cleaned_Shortened, Length: 2089, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85c58295-082f-4856-8e7b-677e8c5d6077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/.pyenv/versions/3.10.6/envs/reddit_nlp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model.save_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "df[\"Comment_wo_links_stopwords_shortened\"] = df[\"Comment_wo_links_stopwords\"].apply(lambda x: reduce_sentence_length(x))\n",
    "df[\"Comment_wo_links_stopwords_shortened_tokens\"] = df[\"Comment_wo_links_stopwords_shortened\"].apply(lambda x: tokenizer(x,return_tensors=\"tf\"))\n",
    "df[\"Output\"] = df[\"Comment_wo_links_stopwords_shortened_tokens\"].apply(lambda x: model(x))\n",
    "df[\"Scores\"] = df[\"Output\"].apply(lambda x: x[0][0].numpy())\n",
    "df[\"Scores\"] = df[\"Scores\"].apply(lambda x: softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ed69635-0700-4484-adf0-768c2ce16c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the label with the highest probability\n",
    "def get_highest_label(scores):\n",
    "    ranking = np.argsort(scores)[::-1]\n",
    "    highest_label_index = ranking[0]\n",
    "    highest_label = model.config.id2label[highest_label_index]\n",
    "    return highest_label\n",
    "\n",
    "# Apply the function to get the highest label for each row\n",
    "df[\"Highest_Label\"] = df[\"Scores\"].apply(lambda x: get_highest_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af7b4fa5-9d6f-4821-8bbf-7e96d2440b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       negative\n",
       "1       negative\n",
       "2       negative\n",
       "3       negative\n",
       "4       negative\n",
       "          ...   \n",
       "2084    negative\n",
       "2085    negative\n",
       "2086    positive\n",
       "2087    positive\n",
       "2088    positive\n",
       "Name: Highest_Label, Length: 2089, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Highest_Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2cf2c1b-48fc-4d3c-84c6-f4cb88ca9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unwrap 'label' from each list element into a new column\n",
    "df['label'] = result.apply(lambda x: x[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fc08fc7-d19f-4247-8865-1085fc1ef2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"score\"] = result.apply(lambda x: x[0]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7551dff2-ef8b-4117-b6b0-ea6fd9afc503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Thread_Title</th>\n",
       "      <th>Thread_Score</th>\n",
       "      <th>Thread_URL</th>\n",
       "      <th>Thread_Num_Comments</th>\n",
       "      <th>Thread_Flair</th>\n",
       "      <th>Thread_Selftext</th>\n",
       "      <th>Comment_Body</th>\n",
       "      <th>Comment_Score</th>\n",
       "      <th>Comment_Author</th>\n",
       "      <th>...</th>\n",
       "      <th>Comment_Body_Tokenized</th>\n",
       "      <th>Comment_wo_links_stopwords</th>\n",
       "      <th>Comment_Body_Cleaned_Shortened</th>\n",
       "      <th>Comment_wo_links_stopwords_shortened</th>\n",
       "      <th>Comment_wo_links_stopwords_shortened_tokens</th>\n",
       "      <th>Output</th>\n",
       "      <th>Scores</th>\n",
       "      <th>Highest_Label</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>US: Hamas formally rejected hostage deal</td>\n",
       "      <td>1588</td>\n",
       "      <td>https://www.jns.org/us-hamas-formally-rejected...</td>\n",
       "      <td>261</td>\n",
       "      <td>Israel/Palestine</td>\n",
       "      <td></td>\n",
       "      <td>NO WAY!!! ðŸ¤¯ðŸ¤¯ðŸ¤¯ /s</td>\n",
       "      <td>893</td>\n",
       "      <td>psychotimelord_</td>\n",
       "      <td>...</td>\n",
       "      <td>[way]</td>\n",
       "      <td>no way s</td>\n",
       "      <td>way</td>\n",
       "      <td>no way s</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>{'logits': ((tf.Tensor(0.09184745, shape=(), d...</td>\n",
       "      <td>[0.36424628, 0.31076685, 0.32498685]</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.382617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>US: Hamas formally rejected hostage deal</td>\n",
       "      <td>1588</td>\n",
       "      <td>https://www.jns.org/us-hamas-formally-rejected...</td>\n",
       "      <td>261</td>\n",
       "      <td>Israel/Palestine</td>\n",
       "      <td></td>\n",
       "      <td>It's almost like hamas was the problem all along.</td>\n",
       "      <td>915</td>\n",
       "      <td>Mhdamas</td>\n",
       "      <td>...</td>\n",
       "      <td>[almost, like, hamas, problem, along]</td>\n",
       "      <td>it s almost like hamas was the problem all along</td>\n",
       "      <td>almost like hamas problem along</td>\n",
       "      <td>it s almost like hamas was the problem all along</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>{'logits': ((tf.Tensor(0.14675869, shape=(), d...</td>\n",
       "      <td>[0.3819531, 0.3057211, 0.31232584]</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.377841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>US: Hamas formally rejected hostage deal</td>\n",
       "      <td>1588</td>\n",
       "      <td>https://www.jns.org/us-hamas-formally-rejected...</td>\n",
       "      <td>261</td>\n",
       "      <td>Israel/Palestine</td>\n",
       "      <td></td>\n",
       "      <td>Terrorists being terroristsâ€¦.\\n\\nShocking</td>\n",
       "      <td>164</td>\n",
       "      <td>HisGibness</td>\n",
       "      <td>...</td>\n",
       "      <td>[terrorist, terrorist, shocking]</td>\n",
       "      <td>terrorists being terrorists shocking</td>\n",
       "      <td>terrorist terrorist shocking</td>\n",
       "      <td>terrorists being terrorists shocking</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>{'logits': ((tf.Tensor(0.23327309, shape=(), d...</td>\n",
       "      <td>[0.40154582, 0.28407127, 0.31438294]</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.352980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>US: Hamas formally rejected hostage deal</td>\n",
       "      <td>1588</td>\n",
       "      <td>https://www.jns.org/us-hamas-formally-rejected...</td>\n",
       "      <td>261</td>\n",
       "      <td>Israel/Palestine</td>\n",
       "      <td></td>\n",
       "      <td>Israel will still get blamed.</td>\n",
       "      <td>405</td>\n",
       "      <td>icenoid</td>\n",
       "      <td>...</td>\n",
       "      <td>[israel, still, get, blamed]</td>\n",
       "      <td>israel will still get blamed</td>\n",
       "      <td>israel still get blamed</td>\n",
       "      <td>israel will still get blamed</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>{'logits': ((tf.Tensor(0.23470455, shape=(), d...</td>\n",
       "      <td>[0.4303391, 0.28448442, 0.28517646]</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.390202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>US: Hamas formally rejected hostage deal</td>\n",
       "      <td>1588</td>\n",
       "      <td>https://www.jns.org/us-hamas-formally-rejected...</td>\n",
       "      <td>261</td>\n",
       "      <td>Israel/Palestine</td>\n",
       "      <td></td>\n",
       "      <td>We didn't expect this from these nice masked f...</td>\n",
       "      <td>66</td>\n",
       "      <td>tenonic</td>\n",
       "      <td>...</td>\n",
       "      <td>[expect, nice, masked, fella]</td>\n",
       "      <td>we didn t expect this from these nice masked f...</td>\n",
       "      <td>expect nice masked fella</td>\n",
       "      <td>we didn t expect this from these nice masked f...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>{'logits': ((tf.Tensor(0.12627333, shape=(), d...</td>\n",
       "      <td>[0.35429296, 0.31449333, 0.3312137]</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.404022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>ForeignPolicy</td>\n",
       "      <td>U.S. warned Hezbollah it can't hold Israel bac...</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.axios.com/2024/06/25/us-warned-hez...</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>The US canâ€™t hold Israel back irrespective.</td>\n",
       "      <td>3</td>\n",
       "      <td>Prior_Analytics</td>\n",
       "      <td>...</td>\n",
       "      <td>[u, hold, israel, back, irrespective]</td>\n",
       "      <td>the us can t hold israel back irrespective</td>\n",
       "      <td>u hold israel back irrespective</td>\n",
       "      <td>the us can t hold israel back irrespective</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>{'logits': ((tf.Tensor(0.12399446, shape=(), d...</td>\n",
       "      <td>[0.3751612, 0.31800425, 0.30683458]</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.379616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>ForeignPolicy</td>\n",
       "      <td>U.S. warned Hezbollah it can't hold Israel bac...</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.axios.com/2024/06/25/us-warned-hez...</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>Why do they think Hezbollah should be afraid o...</td>\n",
       "      <td>2</td>\n",
       "      <td>RandomAndCasual</td>\n",
       "      <td>...</td>\n",
       "      <td>[think, hezbollah, afraid, holden, back, israel]</td>\n",
       "      <td>why do they think hezbollah should be afraid o...</td>\n",
       "      <td>think hezbollah afraid holden back israel</td>\n",
       "      <td>why do they think hezbollah should be afraid o...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>{'logits': ((tf.Tensor(0.16235295, shape=(), d...</td>\n",
       "      <td>[0.38516906, 0.29857755, 0.31625345]</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.358613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2086</th>\n",
       "      <td>ForeignPolicy</td>\n",
       "      <td>Biggest US foreign policy/geopolitical wins in...</td>\n",
       "      <td>13</td>\n",
       "      <td>https://www.reddit.com/r/foreignpolicy/comment...</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>I don't mean to see only negatives, but it app...</td>\n",
       "      <td>Well the United Statesâ€™ biggest win is probabl...</td>\n",
       "      <td>16</td>\n",
       "      <td>Njegos1789</td>\n",
       "      <td>...</td>\n",
       "      <td>[well, united, state, biggest, win, probably, ...</td>\n",
       "      <td>well the united states biggest win is probably...</td>\n",
       "      <td>well united state biggest win probably incorpo...</td>\n",
       "      <td>well the united states biggest win is probably...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>{'logits': ((tf.Tensor(-0.09282714, shape=(), ...</td>\n",
       "      <td>[0.323959, 0.31129536, 0.36474562]</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.369694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>ForeignPolicy</td>\n",
       "      <td>Biggest US foreign policy/geopolitical wins in...</td>\n",
       "      <td>13</td>\n",
       "      <td>https://www.reddit.com/r/foreignpolicy/comment...</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>I don't mean to see only negatives, but it app...</td>\n",
       "      <td>PEPFAR</td>\n",
       "      <td>5</td>\n",
       "      <td>pm_me_ur_bidets</td>\n",
       "      <td>...</td>\n",
       "      <td>[pepfar]</td>\n",
       "      <td>pepfar</td>\n",
       "      <td>pepfar</td>\n",
       "      <td>pepfar</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>{'logits': ((tf.Tensor(0.04491749, shape=(), d...</td>\n",
       "      <td>[0.34096968, 0.28091195, 0.37811837]</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.361505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>ForeignPolicy</td>\n",
       "      <td>Biggest US foreign policy/geopolitical wins in...</td>\n",
       "      <td>13</td>\n",
       "      <td>https://www.reddit.com/r/foreignpolicy/comment...</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>I don't mean to see only negatives, but it app...</td>\n",
       "      <td>agree. Also want to point out the Philippines ...</td>\n",
       "      <td>4</td>\n",
       "      <td>pm_me_ur_bidets</td>\n",
       "      <td>...</td>\n",
       "      <td>[agree, also, want, point, philippine, also, g...</td>\n",
       "      <td>agree also want to point out the philippines a...</td>\n",
       "      <td>agree also want point philippine also greatly ...</td>\n",
       "      <td>agree also want to point out the philippines a...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>{'logits': ((tf.Tensor(0.0082282545, shape=(),...</td>\n",
       "      <td>[0.3315917, 0.3015872, 0.36682114]</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.372185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2089 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Subreddit                                       Thread_Title  \\\n",
       "0         worldnews           US: Hamas formally rejected hostage deal   \n",
       "1         worldnews           US: Hamas formally rejected hostage deal   \n",
       "2         worldnews           US: Hamas formally rejected hostage deal   \n",
       "3         worldnews           US: Hamas formally rejected hostage deal   \n",
       "4         worldnews           US: Hamas formally rejected hostage deal   \n",
       "...             ...                                                ...   \n",
       "2084  ForeignPolicy  U.S. warned Hezbollah it can't hold Israel bac...   \n",
       "2085  ForeignPolicy  U.S. warned Hezbollah it can't hold Israel bac...   \n",
       "2086  ForeignPolicy  Biggest US foreign policy/geopolitical wins in...   \n",
       "2087  ForeignPolicy  Biggest US foreign policy/geopolitical wins in...   \n",
       "2088  ForeignPolicy  Biggest US foreign policy/geopolitical wins in...   \n",
       "\n",
       "      Thread_Score                                         Thread_URL  \\\n",
       "0             1588  https://www.jns.org/us-hamas-formally-rejected...   \n",
       "1             1588  https://www.jns.org/us-hamas-formally-rejected...   \n",
       "2             1588  https://www.jns.org/us-hamas-formally-rejected...   \n",
       "3             1588  https://www.jns.org/us-hamas-formally-rejected...   \n",
       "4             1588  https://www.jns.org/us-hamas-formally-rejected...   \n",
       "...            ...                                                ...   \n",
       "2084             3  https://www.axios.com/2024/06/25/us-warned-hez...   \n",
       "2085             3  https://www.axios.com/2024/06/25/us-warned-hez...   \n",
       "2086            13  https://www.reddit.com/r/foreignpolicy/comment...   \n",
       "2087            13  https://www.reddit.com/r/foreignpolicy/comment...   \n",
       "2088            13  https://www.reddit.com/r/foreignpolicy/comment...   \n",
       "\n",
       "      Thread_Num_Comments      Thread_Flair  \\\n",
       "0                     261  Israel/Palestine   \n",
       "1                     261  Israel/Palestine   \n",
       "2                     261  Israel/Palestine   \n",
       "3                     261  Israel/Palestine   \n",
       "4                     261  Israel/Palestine   \n",
       "...                   ...               ...   \n",
       "2084                    2              None   \n",
       "2085                    2              None   \n",
       "2086                    3              None   \n",
       "2087                    3              None   \n",
       "2088                    3              None   \n",
       "\n",
       "                                        Thread_Selftext  \\\n",
       "0                                                         \n",
       "1                                                         \n",
       "2                                                         \n",
       "3                                                         \n",
       "4                                                         \n",
       "...                                                 ...   \n",
       "2084                                                      \n",
       "2085                                                      \n",
       "2086  I don't mean to see only negatives, but it app...   \n",
       "2087  I don't mean to see only negatives, but it app...   \n",
       "2088  I don't mean to see only negatives, but it app...   \n",
       "\n",
       "                                           Comment_Body  Comment_Score  \\\n",
       "0                                      NO WAY!!! ðŸ¤¯ðŸ¤¯ðŸ¤¯ /s            893   \n",
       "1     It's almost like hamas was the problem all along.            915   \n",
       "2             Terrorists being terroristsâ€¦.\\n\\nShocking            164   \n",
       "3                         Israel will still get blamed.            405   \n",
       "4     We didn't expect this from these nice masked f...             66   \n",
       "...                                                 ...            ...   \n",
       "2084       The US canâ€™t hold Israel back irrespective.Â               3   \n",
       "2085  Why do they think Hezbollah should be afraid o...              2   \n",
       "2086  Well the United Statesâ€™ biggest win is probabl...             16   \n",
       "2087                                             PEPFAR              5   \n",
       "2088  agree. Also want to point out the Philippines ...              4   \n",
       "\n",
       "       Comment_Author  ...                             Comment_Body_Tokenized  \\\n",
       "0     psychotimelord_  ...                                              [way]   \n",
       "1             Mhdamas  ...              [almost, like, hamas, problem, along]   \n",
       "2          HisGibness  ...                   [terrorist, terrorist, shocking]   \n",
       "3             icenoid  ...                       [israel, still, get, blamed]   \n",
       "4             tenonic  ...                      [expect, nice, masked, fella]   \n",
       "...               ...  ...                                                ...   \n",
       "2084  Prior_Analytics  ...              [u, hold, israel, back, irrespective]   \n",
       "2085  RandomAndCasual  ...   [think, hezbollah, afraid, holden, back, israel]   \n",
       "2086       Njegos1789  ...  [well, united, state, biggest, win, probably, ...   \n",
       "2087  pm_me_ur_bidets  ...                                           [pepfar]   \n",
       "2088  pm_me_ur_bidets  ...  [agree, also, want, point, philippine, also, g...   \n",
       "\n",
       "                             Comment_wo_links_stopwords  \\\n",
       "0                                              no way s   \n",
       "1      it s almost like hamas was the problem all along   \n",
       "2                  terrorists being terrorists shocking   \n",
       "3                          israel will still get blamed   \n",
       "4     we didn t expect this from these nice masked f...   \n",
       "...                                                 ...   \n",
       "2084         the us can t hold israel back irrespective   \n",
       "2085  why do they think hezbollah should be afraid o...   \n",
       "2086  well the united states biggest win is probably...   \n",
       "2087                                             pepfar   \n",
       "2088  agree also want to point out the philippines a...   \n",
       "\n",
       "                         Comment_Body_Cleaned_Shortened  \\\n",
       "0                                                   way   \n",
       "1                       almost like hamas problem along   \n",
       "2                          terrorist terrorist shocking   \n",
       "3                               israel still get blamed   \n",
       "4                              expect nice masked fella   \n",
       "...                                                 ...   \n",
       "2084                    u hold israel back irrespective   \n",
       "2085          think hezbollah afraid holden back israel   \n",
       "2086  well united state biggest win probably incorpo...   \n",
       "2087                                             pepfar   \n",
       "2088  agree also want point philippine also greatly ...   \n",
       "\n",
       "                   Comment_wo_links_stopwords_shortened  \\\n",
       "0                                              no way s   \n",
       "1      it s almost like hamas was the problem all along   \n",
       "2                  terrorists being terrorists shocking   \n",
       "3                          israel will still get blamed   \n",
       "4     we didn t expect this from these nice masked f...   \n",
       "...                                                 ...   \n",
       "2084         the us can t hold israel back irrespective   \n",
       "2085  why do they think hezbollah should be afraid o...   \n",
       "2086  well the united states biggest win is probably...   \n",
       "2087                                             pepfar   \n",
       "2088  agree also want to point out the philippines a...   \n",
       "\n",
       "     Comment_wo_links_stopwords_shortened_tokens  \\\n",
       "0                    [input_ids, attention_mask]   \n",
       "1                    [input_ids, attention_mask]   \n",
       "2                    [input_ids, attention_mask]   \n",
       "3                    [input_ids, attention_mask]   \n",
       "4                    [input_ids, attention_mask]   \n",
       "...                                          ...   \n",
       "2084                 [input_ids, attention_mask]   \n",
       "2085                 [input_ids, attention_mask]   \n",
       "2086                 [input_ids, attention_mask]   \n",
       "2087                 [input_ids, attention_mask]   \n",
       "2088                 [input_ids, attention_mask]   \n",
       "\n",
       "                                                 Output  \\\n",
       "0     {'logits': ((tf.Tensor(0.09184745, shape=(), d...   \n",
       "1     {'logits': ((tf.Tensor(0.14675869, shape=(), d...   \n",
       "2     {'logits': ((tf.Tensor(0.23327309, shape=(), d...   \n",
       "3     {'logits': ((tf.Tensor(0.23470455, shape=(), d...   \n",
       "4     {'logits': ((tf.Tensor(0.12627333, shape=(), d...   \n",
       "...                                                 ...   \n",
       "2084  {'logits': ((tf.Tensor(0.12399446, shape=(), d...   \n",
       "2085  {'logits': ((tf.Tensor(0.16235295, shape=(), d...   \n",
       "2086  {'logits': ((tf.Tensor(-0.09282714, shape=(), ...   \n",
       "2087  {'logits': ((tf.Tensor(0.04491749, shape=(), d...   \n",
       "2088  {'logits': ((tf.Tensor(0.0082282545, shape=(),...   \n",
       "\n",
       "                                    Scores Highest_Label     label     score  \n",
       "0     [0.36424628, 0.31076685, 0.32498685]      negative   neutral  0.382617  \n",
       "1       [0.3819531, 0.3057211, 0.31232584]      negative   neutral  0.377841  \n",
       "2     [0.40154582, 0.28407127, 0.31438294]      negative   neutral  0.352980  \n",
       "3      [0.4303391, 0.28448442, 0.28517646]      negative   neutral  0.390202  \n",
       "4      [0.35429296, 0.31449333, 0.3312137]      negative   neutral  0.404022  \n",
       "...                                    ...           ...       ...       ...  \n",
       "2084   [0.3751612, 0.31800425, 0.30683458]      negative   neutral  0.379616  \n",
       "2085  [0.38516906, 0.29857755, 0.31625345]      negative  positive  0.358613  \n",
       "2086    [0.323959, 0.31129536, 0.36474562]      positive   neutral  0.369694  \n",
       "2087  [0.34096968, 0.28091195, 0.37811837]      positive  positive  0.361505  \n",
       "2088    [0.3315917, 0.3015872, 0.36682114]      positive   neutral  0.372185  \n",
       "\n",
       "[2089 rows x 21 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61bb0a05-f969-40bc-b314-1b271a710007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV (optional)\n",
    "df.to_csv('filtered_subreddit_threads_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e3146e-92bc-4341-9e46-09668ae395fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd75cce8-7d22-4ff9-a93e-68f5f5f12123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7636e2-3343-4998-85bb-597ad734ef00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
